{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3\n",
    "from dqn import DQN\n",
    "from ppo import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.dqn.policies import MlpPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyhessian\n",
    "from pyhessian import RL_hessian \n",
    "from pyhessian.density_plot import get_esd_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2523136/9912422 [00:00<00:00, 25213289.98it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 23428413.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 25578318.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4523991.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "torch.Size([60000, 784])\n",
      "cuda:0\n",
      "torch.Size([60000])\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_mnist_data():\n",
    "    transform = torchvision.transforms.Compose(\n",
    "        [torchvision.transforms.ToTensor()])\n",
    "\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"data\", train=True, transform=transform, download=True\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=60000\n",
    "    )\n",
    "\n",
    "    for _, (images, labels) in enumerate(train_loader):\n",
    "        images = images.flatten(start_dim=1)\n",
    "        labels = labels\n",
    "\n",
    "    xd = images\n",
    "    yd = labels\n",
    "    return images.to(device), labels.to(device)\n",
    "x, y = get_mnist_data() \n",
    "print(x.shape)\n",
    "print(x.device)\n",
    "print(y.shape)\n",
    "print(y.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistEnv(gym.Env):\n",
    "    def __init__(self, x, y):\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(10)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(28, 28, 1), dtype=np.float32)\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.shuffle_data()\n",
    "\n",
    "        self.dataset_idx = 0\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        pixel_permutation = np.random.permutation(784)\n",
    "        x_permuted = self.x[:, pixel_permutation]\n",
    "        data_permutation = np.random.permutation(60000)\n",
    "        self.x = x_permuted[data_permutation]\n",
    "        self.y = self.y[data_permutation]\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = int(action == self.expected_action)\n",
    "\n",
    "        if self.dataset_idx == (60000 - 1):\n",
    "            self.dataset_idx = 0\n",
    "            self.shuffle_data()\n",
    "        else:\n",
    "            self.dataset_idx += 1\n",
    "        obs = self._get_obs()\n",
    "        \n",
    "        done = True\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        self.expected_action = int(self.y[self.dataset_idx])\n",
    "        obs = self.x[self.dataset_idx].reshape(28, 28, 1)\n",
    "        return obs.cpu().numpy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eigenvalues = []\n",
    "eigenvalue_densities = []\n",
    "eigenvalue_weights = []\n",
    "mean_rewards = []\n",
    "std_rewards = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_e =  np.array(max_eigenvalues)\n",
    "e_d = np.array(eigenvalue_densities)\n",
    "e_w = np.array(eigenvalue_weights)\n",
    "m_r = np.array(mean_rewards)\n",
    "s_r = np.array(std_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(file, data):\n",
    "    with open(file, 'wb+') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_data(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kek_data = {\n",
    "    'max_ev': m_e,\n",
    "    'ev_densities': e_d,\n",
    "    'ev_weights': e_w,\n",
    "    'avg_reward': m_r,\n",
    "    'reward_std': s_r\n",
    "}\n",
    "datapath = ''\n",
    "save_data(datapath, kek_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_ppo(num_tasks=50):\n",
    "    # logger.configure(dir='./logs/mnist_dqn', format_strs=['stdout', 'tensorboard'])\n",
    "    env = MnistEnv(x, y)\n",
    "    # check_env(env)\n",
    "    # env = bench.Monitor(env, logger.get_dir())\n",
    "\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        n_steps=1000\n",
    "        # learning_rate=.0001,\n",
    "    )\n",
    "\n",
    "    for task_number in tqdm(range(num_tasks)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.learn(total_timesteps=50000, log_interval=4)\n",
    "\n",
    "        learn_time = time.time()\n",
    "\n",
    "        print('starting HESSIAN shit')\n",
    "\n",
    "        hessian = (RL_hessian(model, batch_size=100, cuda=True))\n",
    "        top_eigenvalues, top_eigenvector = hessian.eigenvalues()\n",
    "        max_eigenvalues.append(top_eigenvalues[-1])\n",
    "        density_eigen, density_weight = hessian.density()\n",
    "        eigenvalue_densities.append(density_eigen)\n",
    "        eigenvalue_weights.append(density_weight)\n",
    "        if task_number % 10 == 0:\n",
    "            get_esd_plot(density_eigen, density_weight, name=str(task_number))\n",
    "\n",
    "        hessian_time = time.time() \n",
    "\n",
    "        print('starting EVAL shit')\n",
    "\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10000, deterministic=False)\n",
    "\n",
    "        eval_time = time.time() \n",
    "\n",
    "        print(f\"Train time: {learn_time - start_time}, hessian time: {hessian_time - learn_time},  eval time: {eval_time - hessian_time}\")\n",
    "\n",
    "        mean_rewards.append(mean_reward)\n",
    "        std_rewards.append(std_reward)\n",
    "\n",
    "        print(f\"Task: {task_number},  Reward: {mean_reward},  Std: {std_reward}\")\n",
    "\n",
    "        model.save('ppo_mnist_1.pkl')\n",
    "    \n",
    "mnist_ppo(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomMLPPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = MnistEnv(x, y)\n",
    "# check_env(env)\n",
    "# env = bench.Monitor(env, logger.get_dir())\n",
    "\n",
    "model = DQN(\n",
    "    CustomMLPPolicy,\n",
    "    env,\n",
    "    buffer_size=10000,\n",
    "    target_update_interval=1000,\n",
    "    learning_starts=100,\n",
    "    exploration_final_eps=.001,\n",
    "    policy_kwargs=dict(optimizer_class=torch.optim.AdamW), \n",
    ")\n",
    "print(model.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLPPolicy(MlpPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomMLPPolicy, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def renormalize_qnet_layer(self, top10_eigenvectors, top10_eigenvalues, layer_num):\n",
    "        W = self.q_net.q_net[layer_num].weight.data\n",
    "        eigenvectors = [eigenvector[layer_num] for eigenvector in top10_eigenvectors]\n",
    "\n",
    "        for e, lambda_val in zip(eigenvectors, top10_eigenvalues):\n",
    "            # Ensure e is a tensor and has the same device as W\n",
    "            e = e.to(device)\n",
    "\n",
    "            # Normalize the eigenvector\n",
    "            e_normalized = e / torch.norm(e)\n",
    "            \n",
    "            # Compute projection of W onto e\n",
    "            projection = torch.dot(W.view(-1), e_normalized.view(-1)) * e_normalized\n",
    "            # Subtract the scaled projection from W\n",
    "            W -= lambda_val * projection.view_as(W)\n",
    "\n",
    "        # Update the weights of the layer\n",
    "        self.q_net.q_net[layer_num].weight.data = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]c:\\Users\\justi\\cs285-rl-plasticity\\Lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ..\\torch\\csrc\\autograd\\engine.cpp:1176.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\justi\\cs285-rl-plasticity\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/200 [00:39<2:11:47, 39.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: 0,  Reward: 0.206,  Std: 0.4044304637388237\n",
      "Train time: 1.1146998405456543, hessian time: 25.290683031082153,  eval time: 13.319026708602905, reinit time: 0.012491226196289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:42<2:22:18, 42.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\rl_classification.ipynb Cell 18\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         reinit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain time: \u001b[39m\u001b[39m{\u001b[39;00mlearn_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m}\u001b[39;00m\u001b[39m, hessian time: \u001b[39m\u001b[39m{\u001b[39;00mhessian_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mlearn_time\u001b[39m}\u001b[39;00m\u001b[39m,  eval time: \u001b[39m\u001b[39m{\u001b[39;00meval_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mhessian_time\u001b[39m}\u001b[39;00m\u001b[39m, reinit time: \u001b[39m\u001b[39m{\u001b[39;00mreinit_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39meval_time\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m mnist_dqn(\u001b[39m200\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\rl_classification.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m hessian \u001b[39m=\u001b[39m (RL_hessian(model, batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, cuda\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m top_eigenvalues, top_eigenvector \u001b[39m=\u001b[39m hessian\u001b[39m.\u001b[39;49meigenvalues(top_n\u001b[39m=\u001b[39;49mn)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m xd \u001b[39m=\u001b[39m top_eigenvector\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/justi/cs285-rl-plasticity/rl_classification.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m max_eigenvalues\u001b[39m.\u001b[39mappend(top_eigenvalues[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\pyhessian\\RL_hessian.py:121\u001b[0m, in \u001b[0;36mRL_hessian.eigenvalues\u001b[1;34m(self, maxIter, tol, top_n)\u001b[0m\n\u001b[0;32m    118\u001b[0m Hv \u001b[39m=\u001b[39m hessian_vector_product(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradsH, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams, v)\n\u001b[0;32m    119\u001b[0m tmp_eigenvalue \u001b[39m=\u001b[39m group_product(Hv, v)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m--> 121\u001b[0m v \u001b[39m=\u001b[39m normalization(Hv)\n\u001b[0;32m    123\u001b[0m \u001b[39mif\u001b[39;00m eigenvalue \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     eigenvalue \u001b[39m=\u001b[39m tmp_eigenvalue\n",
      "File \u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\pyhessian\\utils.py:54\u001b[0m, in \u001b[0;36mnormalization\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalization\u001b[39m(v):\n\u001b[0;32m     50\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    normalization of a list of vectors\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[39m    return: normalized vectors v\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     s \u001b[39m=\u001b[39m group_product(v, v)\n\u001b[0;32m     55\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m0.5\u001b[39m\n\u001b[0;32m     56\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\pyhessian\\utils.py:34\u001b[0m, in \u001b[0;36mgroup_product\u001b[1;34m(xs, ys)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_product\u001b[39m(xs, ys):\n\u001b[0;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    the inner product of two lists of variables xs,ys\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    :param xs:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    :param ys:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m([torch\u001b[39m.\u001b[39;49msum(x \u001b[39m*\u001b[39;49m y) \u001b[39mfor\u001b[39;49;00m (x, y) \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(xs, ys)])\n",
      "File \u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\pyhessian\\utils.py:34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgroup_product\u001b[39m(xs, ys):\n\u001b[0;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m    the inner product of two lists of variables xs,ys\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m    :param xs:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m    :param ys:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39m    :return:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msum\u001b[39m([torch\u001b[39m.\u001b[39;49msum(x \u001b[39m*\u001b[39;49m y) \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(xs, ys)])\n",
      "File \u001b[1;32mc:\\Users\\justi\\cs285-rl-plasticity\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mnist_dqn(num_tasks=50):\n",
    "    # logger.configure(dir='./logs/mnist_dqn', format_strs=['stdout', 'tensorboard'])\n",
    "    env = MnistEnv(x, y)\n",
    "    # check_env(env)\n",
    "    # env = bench.Monitor(env, logger.get_dir())\n",
    "\n",
    "    model = DQN(\n",
    "        CustomMLPPolicy,\n",
    "        env,\n",
    "        buffer_size=10000,\n",
    "        target_update_interval=1000,\n",
    "        learning_starts=100,\n",
    "        exploration_final_eps=.001,\n",
    "        policy_kwargs=dict(optimizer_class=torch.optim.AdamW), \n",
    "    )\n",
    "\n",
    "    for task_number in tqdm(range(num_tasks)):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.learn(total_timesteps=500, log_interval=4)\n",
    "\n",
    "        learn_time = time.time()\n",
    "\n",
    "        n = 10\n",
    "\n",
    "        hessian = (RL_hessian(model, batch_size=100, cuda=True))\n",
    "        top_eigenvalues, top_eigenvector = hessian.eigenvalues(top_n=n)\n",
    "        xd = top_eigenvector\n",
    "        max_eigenvalues.append(top_eigenvalues[-1])\n",
    "        density_eigen, density_weight = hessian.density()\n",
    "        eigenvalue_densities.append(density_eigen)\n",
    "        eigenvalue_weights.append(density_weight)\n",
    "        if task_number % 10 == 0:\n",
    "            get_esd_plot(density_eigen, density_weight, name=str(task_number))\n",
    "\n",
    "        hessian_time = time.time() \n",
    "\n",
    "        mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10000, deterministic=False)\n",
    "\n",
    "        eval_time = time.time() \n",
    "\n",
    "        mean_rewards.append(mean_reward)\n",
    "        std_rewards.append(std_reward)\n",
    "\n",
    "        print(f\"Task: {task_number},  Reward: {mean_reward},  Std: {std_reward}\")\n",
    "\n",
    "        # model.save('dpo_mnist_testing.pkl')\n",
    "        model.policy.renormalize_qnet_layer(top_eigenvector, top_eigenvalues, 2)\n",
    "\n",
    "        reinit_time = time.time()\n",
    "\n",
    "        print(f\"Train time: {learn_time - start_time}, hessian time: {hessian_time - learn_time},  eval time: {eval_time - hessian_time}, reinit time: {reinit_time - eval_time}\")\n",
    "        \n",
    "    \n",
    "mnist_dqn(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
